{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load data - for data loading we have two options:\n",
    "1. Load file with e-code, disease and one result\n",
    "2. Load file with e-code, disease and multiple results (mapped with corresponding NM codes)"
   ],
   "id": "a46939117b37865d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_phenotype_file(filepath):\n",
    "    # Read the TSV file with UTF-8 encoding\n",
    "    df = pd.read_csv(filepath, delimiter='\\t', header=None, encoding='ISO-8859-1')\n",
    "    df.columns = ['E_code', 'Disease', 'Result']  # Setting column names according to the expected format\n",
    "\n",
    "    # Replace any nan values in 'Result' with 'NA'\n",
    "    df['Result'] = df['Result'].fillna('NA')\n",
    "\n",
    "    # Store Disease and Result in a tuple for each E_code\n",
    "    aggregated_dict = {row['E_code']: (row['Disease'], row['Result']) for index, row in df.iterrows()}\n",
    "    print(\"Aggregated data parsed successfully.\")\n",
    "    print(aggregated_dict)\n",
    "    return aggregated_dict\n",
    "\n",
    "phenotype_data= parse_phenotype_file('/mnt/sdb/phenotype-globals-tshc-7500.tsv') # file for one result per e-code"
   ],
   "id": "249589fdb6712ca7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_nm_identifiers(expression):\n",
    "    if pd.isna(expression):\n",
    "        return []\n",
    "    else:\n",
    "        # Extract NM and other identifiers using regex\n",
    "        return re.findall(r'(?:LRG_\\d+t\\d+|NM_\\d+\\.\\d+)(?::c\\.[^\\s,]+)?', expression)\n",
    "\n",
    "def parse_phenotype_file_HGSV(filepath, additional_nucleotide=True):\n",
    "    # Load data\n",
    "    data = pd.read_csv(filepath, sep='\\t', encoding='utf-8')\n",
    "\n",
    "    # Create a dictionary to store extracted data\n",
    "    extracted_data = {}\n",
    "\n",
    "    # Iterate over rows\n",
    "    for index, row in data.iterrows():\n",
    "        e_code = row['E_code']\n",
    "        disease = row['Disease']\n",
    "        symbol = row['#Symbol']\n",
    "        nucleotide_expression = row['NucleotideExpression']\n",
    "        additional_nucleotide_expressions = row['Additional_NucleotideExpressions']\n",
    "\n",
    "        # Extract NM identifiers\n",
    "        nm_identifiers = extract_nm_identifiers(additional_nucleotide_expressions)\n",
    "\n",
    "        # Ensure unique and ordered identifiers to avoid duplication\n",
    "        unique_identifiers = sorted(set(nm_identifiers), key=lambda x: x.split(':c.')[1] if ':c.' in x else x)\n",
    "\n",
    "        # Store the extracted data, changing from list to tuple\n",
    "        if unique_identifiers and additional_nucleotide:\n",
    "            extracted_data[e_code] = (disease, symbol, nucleotide_expression, *unique_identifiers)\n",
    "        else:\n",
    "            extracted_data[e_code] = (disease, symbol, nucleotide_expression)\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "# Example usage\n",
    "file_path = '/mnt/sdb/markus-bsc-thesis-data/cleaned-HGSV-phenotype-globals-tshc-7500.tsv'\n",
    "phenotype_data = parse_phenotype_file_HGSV(file_path, True)\n",
    "\n",
    "# Print only the first 20 entries from the extracted data\n",
    "for i, (e_code, info) in enumerate(phenotype_data.items()):\n",
    "    if i >= 20:\n",
    "        break\n",
    "    print(f\"{e_code}: {info}\")"
   ],
   "id": "436997bbab5407f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare data for classification and converting them to TSV files ",
   "id": "4a30650c21e40822"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_csq_final(csq_list, alleles_list):\n",
    "    parsed_data = []\n",
    "    for csq, alleles in zip(csq_list, alleles_list):\n",
    "        # Handle the case where csq is a list and not a string\n",
    "        csq_str = '|'.join(csq) if isinstance(csq, list) else csq  # Join list into a single string if it's a list\n",
    "        csq_str = csq_str.strip('[\"]') if csq_str else csq_str  # Ensure csq_str is not None before calling strip\n",
    "        parts = csq_str.split(\"|\") if csq_str else []\n",
    "\n",
    "        impact = parts[0] if len(parts) > 0 else \"Unknown\"\n",
    "        symbol = parts[1] if len(parts) > 1 else \"Unknown\"\n",
    "        hgnc_id = parts[2] if len(parts) > 2 and parts[2].isdigit() else None\n",
    "        max_af = float(parts[3]) if len(parts) > 3 and re.match(r'^-?\\d*\\.?\\d+(?:[Ee][-+]?\\d+)?$', parts[3]) else 0\n",
    "\n",
    "        nm_matches = \", \".join(re.findall(r'NM_\\d+\\.\\d+', csq_str))\n",
    "        c_notations = \", \".join(\n",
    "            [notation.split(':')[-1] for notation in re.findall(r'NM_\\d+\\.\\d+:c\\.[\\d+-]+[ACTG]>[ACTG]', csq_str)])\n",
    "\n",
    "        # Handle the case where alleles might also be a list\n",
    "        alleles_str = '|'.join(alleles) if isinstance(alleles, list) else alleles\n",
    "        alleles_str = alleles_str.strip('[]').replace('\"', '').replace(' ', '') if alleles_str else alleles_str\n",
    "\n",
    "        parsed_data.append({\n",
    "            \"IMPACT\": impact,\n",
    "            \"SYMBOL\": symbol,\n",
    "            \"HGNC_ID\": hgnc_id,\n",
    "            \"MAX_AF\": max_af,\n",
    "            \"NM_STRING\": nm_matches,\n",
    "            \"MUTATIONS\": c_notations,\n",
    "            \"alleles\": alleles_str\n",
    "        })\n",
    "    return parsed_data"
   ],
   "id": "e4b3c40aaac4d0ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def classify_disease(disease, position):\n",
    "    if disease == 'RV' and (position != 'NA' and position != 'NEG'):\n",
    "        return 'positive-group-RV'\n",
    "    else:\n",
    "        return 'negative-group'\n",
    "\n",
    "def parse_empty(text):\n",
    "    \"\"\"Converts an empty string to a missing value, otherwise converts to integer.\"\"\"\n",
    "    return hl.if_else((text == \"\") | hl.is_missing(text) | ~text.matches(r\"\\d+\"), hl.missing(hl.tint32), hl.int32(text))\n",
    "\n",
    "\n",
    "def safe_float_parse(s):\n",
    "    \"\"\"Parse a float safely, return None if parsing fails due to non-numeric string.\"\"\"\n",
    "    return hl.if_else(hl.is_defined(s) & s.matches(r'^-?\\d*(\\.\\d+)?$'), hl.float64(s), hl.missing(hl.tfloat64))\n",
    "\n",
    "\n",
    "def parse_to_int32(text):\n",
    "    return hl.if_else((text == \"\") | (text == \"null\"), hl.missing(hl.tint32), hl.int32(text))\n",
    "\n",
    "\n",
    "def parse_to_float64(text):\n",
    "    # This regex will match numbers including integers, floats, and also consider negative values\n",
    "    return hl.if_else(hl.is_defined(text) & text.matches(r'^-?\\d*\\.?\\d+(?:[Ee][-+]?\\d+)?$'), hl.float64(text),\n",
    "                      hl.missing(hl.tfloat64))\n",
    "\n",
    "\n",
    "def safe_index(split_list, index, default=None):\n",
    "    try:\n",
    "        return split_list[index]\n",
    "    except IndexError:\n",
    "        return default\n",
    "\n",
    "\n",
    "def vcfs_to_matrixtable(source, phenotype_data, base_destination, write=True,\n",
    "                        log_file='/home/markus/gen-toolbox/output/processed_vcfs_log.tsv'):\n",
    "    files = []\n",
    "\n",
    "    if os.path.isdir(source):\n",
    "        files = [os.path.join(source, f) for f in os.listdir(source) if f.endswith('.vcf') or f.endswith('.vcf.gz')]\n",
    "    elif os.path.isfile(source) and (source.endswith('.vcf') or source.endswith('.vcf.gz')):\n",
    "        files.append(source)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid path or file type. Must be a directory or a VCF file.\")\n",
    "\n",
    "    hl.init(default_reference='GRCh37')  # Initialize Hail\n",
    "    contig_recoding = {f\"chr{i}\": str(i) for i in range(1, 23)}\n",
    "    contig_recoding.update({\"chrX\": \"X\", \"chrY\": \"Y\"})\n",
    "\n",
    "    log_entries = []\n",
    "    processed_vcfs = []\n",
    "\n",
    "    # Import and annotate files in a loop\n",
    "    try:\n",
    "        for vcf in tqdm(files, desc=\"Processing VCFs\"):\n",
    "\n",
    "            patient_code = os.path.basename(vcf).split('_')[0]\n",
    "            phenotype_info = phenotype_data.get(patient_code, (\"NA\", \"NA\"))\n",
    "\n",
    "            disease, position = phenotype_info\n",
    "            group = classify_disease(disease, position)\n",
    "\n",
    "            destination = os.path.join(base_destination, group)\n",
    "            destination_path = os.path.join(destination, os.path.basename(vcf).replace('.vcf', '.mt'))\n",
    "            tsv_output_path = os.path.join(destination, f\"{patient_code}_mtoutput.tsv\")\n",
    "\n",
    "            if os.path.exists(destination_path) or os.path.exists(tsv_output_path):\n",
    "                print(f\"Skipping {vcf}, as the output file already exists in {destination_path}.\")\n",
    "                log_entries.append({\n",
    "                    \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"VCF\": vcf,\n",
    "                    \"Status\": \"Skipped\",\n",
    "                    \"Reason\": \"Output file already exists\",\n",
    "                    \"Path\": destination_path\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            if not os.path.exists(destination):\n",
    "                os.makedirs(destination)\n",
    "\n",
    "            mt = hl.import_vcf(vcf, force_bgz=True, reference_genome='GRCh37', contig_recoding=contig_recoding,\n",
    "                               skip_invalid_loci=True)\n",
    "            mt = mt.filter_rows(mt.alleles[1] != \"*\")  # Filter out star alleles\n",
    "            print(f\"Rows before filtering: {mt.count_rows()}\")\n",
    "            mt = mt.filter_rows((hl.len(mt.filters) == 0) | mt.filters.contains(hl.literal(\"PASS\")))  \n",
    "\n",
    "            print(f\"Rows after filtering PASS: {mt.count_rows()}\")\n",
    "            # Parse CSQ string and annotate rows\n",
    "            # First, make sure `vep` contains at least one element\n",
    "            # Annotate rows with the `vep` field\n",
    "            mt = mt.annotate_rows(\n",
    "                vep=mt.info.CSQ.map(lambda csq: hl.struct(\n",
    "                    IMPACT=safe_index(csq.split(\"|\"), 0, \"Unknown\"),\n",
    "                    SYMBOL=safe_index(csq.split(\"|\"), 1, \"Unknown\"),\n",
    "                    HGNC_ID=hl.if_else(safe_index(csq.split(\"|\"), 2, \"\").matches(r'^-?\\d+$'),\n",
    "                                       parse_to_int32(safe_index(csq.split(\"|\"), 2)), hl.missing(hl.tint32)),\n",
    "                    MAX_AF=parse_to_float64(safe_index(csq.split(\"|\"), 3, \"0\")),\n",
    "                )),\n",
    "                CHROM=mt.locus.contig,\n",
    "                REF=mt.alleles[0],\n",
    "                ALT=mt.alleles[1]\n",
    "            )\n",
    "\n",
    "            # Only keep rows where `vep` is not empty\n",
    "            mt = mt.filter_rows(hl.len(mt.vep) > 0)\n",
    "            # Flatten the first struct in the `vep` array directly into the row fields\n",
    "            mt = mt.transmute_rows(\n",
    "                IMPACT=mt.vep[0].IMPACT,\n",
    "                SYMBOL=mt.vep[0].SYMBOL,\n",
    "                HGNC_ID=mt.vep[0].HGNC_ID,\n",
    "                MAX_AF=mt.vep[0].MAX_AF\n",
    "            )\n",
    "            #mt.info.CSQ.show()\n",
    "            # Define the TSV output path using the patient code and group\n",
    "            df = mt.rows().flatten().to_pandas()\n",
    "            # Process and filter necessary fields with `parse_csq_final`\n",
    "            processed_data = parse_csq_final(df['info.CSQ'].tolist(), df['alleles'].tolist())\n",
    "            filtered_df = pd.DataFrame(processed_data)\n",
    "            # Add the columns to the DataFrame\n",
    "            filtered_df['CHROM'] = df['CHROM']\n",
    "            filtered_df['REF'] = df['alleles'].apply(lambda x: x[0])\n",
    "            filtered_df['ALT'] = df['alleles'].apply(lambda x: x[1] if len(x) > 1 else None)\n",
    "            filtered_df['QUAL'] = df['qual']\n",
    "            for col in df.columns:\n",
    "                if col.startswith('info.') and not col.endswith('CSQ'):\n",
    "                    filtered_df[col.split('.')[1]] = df[col].apply(\n",
    "                        lambda x: x[0] if isinstance(x, list) and len(x) > 0 else x)\n",
    "\n",
    "            if write:\n",
    "                # Define the TSV output path and export\n",
    "                tsv_output_path = os.path.join(destination, f\"{patient_code}_mtoutput.tsv\")\n",
    "                filtered_df.to_csv(tsv_output_path, sep='\\t', index=False)\n",
    "                processed_vcfs.append({'vcf': vcf, 'mt_path': destination_path})\n",
    "                log_entries.append({\n",
    "                    \"Timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                    \"VCF\": vcf,\n",
    "                    \"Status\": \"Processed\",\n",
    "                    \"Reason\": \"Successfully processed and saved\",\n",
    "                    \"Path\": tsv_output_path\n",
    "                })\n",
    "\n",
    "\n",
    "    finally:\n",
    "        hl.stop()  # Stop Hail context when done\n",
    "        log_df = pd.DataFrame(log_entries)\n",
    "        log_df.to_csv(log_file, sep='\\t', index=False)\n",
    "\n",
    "    return processed_vcfs, log_entries\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "VEP_CONFIG_PATH = '/home/markus/gen-toolbox/src/config/vep_settings.json'\n",
    "SOURCE_DIR = '/mnt/sdb/TSHC_data_5k'\n",
    "base_destination_directory = '/mnt/sdb/all-data-markus-bsc-thesis-data-v2'\n",
    "phenotype_data = parse_phenotype_file('/mnt/sdb/phenotype-globals-tshc-7500.tsv') # file for one result per e-code\n",
    "log_entries = vcfs_to_matrixtable(SOURCE_DIR, phenotype_data, base_destination_directory,\n",
    "                                  write=True)\n",
    "print(log_entries)"
   ],
   "id": "9492fc29c5a26541",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data aggregation to create positive and negative group aggregated TSV table",
   "id": "ab2c58e62d8500b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# regex search from TSV files\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def valid_mutation_format(mutation):\n",
    "    \"\"\" Check if the mutation string format is valid. Only proceed if mutation is a string. \"\"\"\n",
    "    if not isinstance(mutation, str):\n",
    "        return False\n",
    "    pattern = re.compile(r'NM_\\d+\\.\\d+:c.*', re.IGNORECASE)\n",
    "    return bool(pattern.match(mutation))\n",
    "\n",
    "def extract_gene_and_mutation(mutation_info):\n",
    "    try:\n",
    "        gene_name, mutation_detail = mutation_info.split(':')\n",
    "        mutation_pattern = re.compile(r'c\\.\\d+[+-]?\\d*(?:[GATC]>[GATC]|del[ACTG]+|ins[ACTG]+|\\d+_\\d+del|\\d+_\\d+ins[ACTG]+)', re.IGNORECASE)\n",
    "        matches = mutation_pattern.search(mutation_detail.strip())  # strip to remove any leading/trailing spaces\n",
    "        if matches:\n",
    "            return gene_name, matches.group()\n",
    "        return gene_name, None\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "\n",
    "def aggregate_tsv_files(base_path, phenotype_dict, process_negative_group=True, process_positive_group=True, symbol=True, mutation=True):\n",
    "    negative_group_path = os.path.join(base_path, 'negative-group')\n",
    "    positive_group_path = os.path.join(base_path, 'positive-group-RV')\n",
    "    neg_dfs = []\n",
    "    pos_dfs = []\n",
    "    val_dfs = []\n",
    "\n",
    "    if process_negative_group:\n",
    "        neg_files = os.listdir(negative_group_path)\n",
    "        for file in tqdm(neg_files, desc=\"Processing Negative Group\"):\n",
    "            file_path = os.path.join(negative_group_path, file)\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "            e_code = file.split('_')[0]\n",
    "            df['E_code'] = e_code\n",
    "            neg_dfs.append(df)\n",
    "\n",
    "    if process_positive_group:\n",
    "        pos_files = os.listdir(positive_group_path)\n",
    "        for file in tqdm(pos_files, desc=\"Processing Positive Group\"):\n",
    "            file_path = os.path.join(positive_group_path, file)\n",
    "            df = pd.read_csv(file_path, sep='\\t', low_memory=False)\n",
    "            e_code = file.split('_')[0]\n",
    "            df['E_code'] = e_code\n",
    "\n",
    "            if e_code in phenotype_dict:\n",
    "                _, _symbol, *mutations = phenotype_dict[e_code]\n",
    "                mutations = [m for m in mutations if valid_mutation_format(m)]\n",
    "                if mutations:  # Only proceed if there are valid mutations\n",
    "                    subtracted_regex = None\n",
    "                    if not mutation: # exclude mutations \n",
    "                        mutation_regex = '|'.join(re.escape(m.split(\":\")[0]) for m in mutations) \n",
    "                    else:\n",
    "                        mutation_regex = '|'.join(re.escape(m) for m in mutations)\n",
    "                        subtracted_mutation_regex = '|'.join(re.escape(m[:-2]) for m in mutations) # Pattern after removing last string from mutation\n",
    "                        subtracted_regex = re.compile(subtracted_mutation_regex, re.IGNORECASE)\n",
    "                        \n",
    "\n",
    "                    regex = re.compile(mutation_regex, re.IGNORECASE)\n",
    "                    \n",
    "                    mask = df['MUTATIONS'].astype(str).apply(lambda x: subtracted_regex.search(x) is not None if regex.search(x) is None and mutation else regex.search(x) is not None)\n",
    "                    df[\"nucleotide_expression\"] = df[\"NM_STRING\"] + \":\" + df[\"MUTATIONS\"] # Combine expression and mutation\n",
    "                    df[\"secondary_matched\"] = False # Field to track if mutation matcches the subtracted regex pattern\n",
    "                    \n",
    "                    if symbol and _symbol: # If symbol is true and e_code symbol is not null\n",
    "                       df[\"matched\"] = (df[\"nucleotide_expression\"].str.contains(regex.pattern, na=False) & (df[\"SYMBOL\"] == _symbol))\n",
    "                       if subtracted_regex:\n",
    "                           df[\"secondary_matched\"] = (df[\"nucleotide_expression\"].str.contains(subtracted_regex.pattern, na=False) & (df[\"SYMBOL\"] == _symbol))\n",
    "                    else:\n",
    "                        df[\"matched\"] = df[\"nucleotide_expression\"].astype(str).apply(lambda x: regex.search(x) is not None)\n",
    "                        if subtracted_regex:\n",
    "                           df[\"secondary_matched\"] = df[\"nucleotide_expression\"].astype(str).apply(lambda x: regex.search(x) is not None)\n",
    "                    \n",
    "                    target_rows = df[(df[\"matched\"] | ((df[\"matched\"] == False) & (df[\"secondary_matched\"]) & (df[\"IMPACT\"] == \"HIGH\")))] # rows that match the expression\n",
    "                    if not target_rows.empty:\n",
    "                        df.loc[mask, 'E_code'] = e_code\n",
    "                        pos_dfs.append(target_rows)\n",
    "                    else:\n",
    "                        # If no matching mutation is found, consider it for validation group\n",
    "                        val_dfs.append(df)\n",
    "\n",
    "    # Aggregate and save DataFrames as done previously\n",
    "    # Combine all DataFrames for the negative group and save\n",
    "    if neg_dfs:\n",
    "        negative_agg_df = pd.concat(neg_dfs, ignore_index=True)\n",
    "        #negative_agg_df.drop([\"nucleotide_expression\", \"matched\"], inplace=True, axis=1)\n",
    "        negative_agg_df.to_csv(os.path.join(base_path, 'negative_group_aggregated.tsv'), sep='\\t', index=False)\n",
    "        print(\"Negative group data saved successfully.\")\n",
    "\n",
    "    # Combine all DataFrames for the positive group and save if there are any\n",
    "    if pos_dfs:\n",
    "        positive_agg_df = pd.concat(pos_dfs, ignore_index=True)\n",
    "        positive_agg_df.drop([\"nucleotide_expression\", \"matched\", \"secondary_matched\"], inplace=True, axis=1)\n",
    "        positive_agg_df.to_csv(os.path.join(base_path, 'positive_group_aggregated.tsv'), sep='\\t', index=False)\n",
    "        print(\"Positive group data saved successfully.\")\n",
    "        \n",
    "    if val_dfs:\n",
    "        print(\"Aga siia\")\n",
    "        validation_agg_df = pd.concat(val_dfs, ignore_index=True)\n",
    "        validation_agg_df.drop([\"nucleotide_expression\", \"matched\", \"secondary_matched\"], inplace=True, axis=1)\n",
    "        validation_agg_df.to_csv(os.path.join(base_path, 'validation_group_aggregated.tsv'), sep='\\t', index=False)\n",
    "        print(\"Validation group data saved successfully.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No data for positive group or all data were non-matching.\")\n",
    "\n",
    "\n",
    "def regex_search(symbol=True, mutation=True, additional_nucleotide_expressions=True):\n",
    "    # Assuming phenotype_data is loaded properly\n",
    "    phenotype_data = parse_phenotype_file_HGSV('/mnt/sdb/markus-bsc-thesis-data/cleaned-HGSV-phenotype-globals-tshc-7500.tsv', additional_nucleotide_expressions)\n",
    "    aggregate_tsv_files('/mnt/sdb/markus-bsc-thesis-data/', phenotype_data, process_negative_group=True, process_positive_group=True, symbol=symbol, mutation=mutation)\n",
    "\n",
    "regex_search(symbol=True, mutation=True, additional_nucleotide_expressions=True)"
   ],
   "id": "c3b9578dd35c7c26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7e1763f02bb4beb2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
