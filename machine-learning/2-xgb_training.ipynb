{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "import os\n",
    "import graphviz\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "ba29729184724118",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prep only most feasible features and do not scale",
   "id": "25519ec84c85ed53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t', na_values=['NA', 'null', ''], low_memory=False)\n",
    "    return df\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(df):\n",
    "    # Creating a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "\n",
    "    # Selecting the relevant columns\n",
    "    relevant_columns = ['IMPACT', 'QUAL', 'DP', 'QD', 'MAX_AF']\n",
    "    df = df[relevant_columns]\n",
    "\n",
    "    # Impact mapping using .loc to avoid SettingWithCopyWarning\n",
    "    impact_mapping = {'HIGH': 0, 'MODERATE': 1, 'LOW': 2, 'MODIFIER': 3}\n",
    "    df.loc[:, 'IMPACT'] = df['IMPACT'].map(impact_mapping)\n",
    "    \n",
    "    # Handle missing values for 'DP' (Depth of coverage)\n",
    "    if df['DP'].isnull().any():\n",
    "        print(\"NaN values found in DP. Imputing with median.\")\n",
    "        dp_imputer = SimpleImputer(strategy='median')\n",
    "        df['DP'] = dp_imputer.fit_transform(df['DP'].values.reshape(-1, 1))\n",
    "\n",
    "    # Handle missing values for 'MAX_AF' (Maximum Allele Frequency)\n",
    "    if df['MAX_AF'].isnull().any():\n",
    "        max_af_imputer = SimpleImputer(strategy='median')\n",
    "        df['MAX_AF'] = max_af_imputer.fit_transform(df[['MAX_AF']])\n",
    "    \n",
    "    # Handling numeric columns 'QUAL' and 'QD'\n",
    "    numeric_cols = ['QUAL', 'QD']\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            print(f\"NaN values found in {col}. Imputing with median.\")\n",
    "            imputer = SimpleImputer(strategy='median')\n",
    "            df[col] = imputer.fit_transform(df[[col]])\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_files_dynamically(base_directory):\n",
    "    for file_name in os.listdir(base_directory):\n",
    "        if file_name.endswith('_aggregated.tsv'):\n",
    "            group_subfolder = None\n",
    "            if 'positive' in file_name:\n",
    "                group_subfolder = 'positive_group'\n",
    "            elif 'negative' in file_name:\n",
    "                group_subfolder = 'negative_group'\n",
    "            elif 'validation' in file_name:\n",
    "                group_subfolder = 'validation_group'\n",
    "\n",
    "            if group_subfolder:\n",
    "                # Create the group directory if it doesn't exist\n",
    "                group_directory = os.path.join(base_directory, group_subfolder)\n",
    "                os.makedirs(group_directory, exist_ok=True)\n",
    "                \n",
    "                # Create the scaled subdirectory within the group directory\n",
    "                scaled_directory = os.path.join(group_directory, 'scaled')\n",
    "                os.makedirs(scaled_directory, exist_ok=True)\n",
    "\n",
    "                # Define input and output file paths\n",
    "                input_file_path = os.path.join(base_directory, file_name)\n",
    "                output_file_name = 'ML_prepped_' + file_name\n",
    "                output_file_path = os.path.join(scaled_directory, output_file_name)\n",
    "                process_and_save_file(input_file_path, output_file_path)\n",
    "\n",
    "def process_and_save_file(input_file_path, output_file_path):\n",
    "    df = load_data(input_file_path)\n",
    "    df_processed = preprocess_data(df)\n",
    "    df_processed.to_csv(output_file_path, index=False, sep='\\t')\n",
    "    print(f\"Data processed and saved to {output_file_path}\")\n",
    "\n",
    "# Example call to the function\n",
    "base_directory = '/mnt/sdb/markus-bsc-thesis-data/machine-learning'\n",
    "process_files_dynamically(base_directory)"
   ],
   "id": "6b4b1a89cfe9eeb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Divide Data into Training Testing and Validation Sets (using positive, negative, and validation groups). And suffle ",
   "id": "4fdd282edf59f4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_data(base_dir, group_name):\n",
    "    tsv_dir = os.path.join(base_dir, group_name, 'scaled')\n",
    "    print(f\"Loading data from {tsv_dir}\")\n",
    "    files = glob(os.path.join(tsv_dir, \"*.tsv\"))\n",
    "    data_list = [pd.read_csv(file, sep='\\t') for file in files]\n",
    "    if data_list:\n",
    "        data = pd.concat(data_list)\n",
    "        data['group'] = group_name\n",
    "    else:\n",
    "        data = pd.DataFrame()\n",
    "    return data\n",
    "\n",
    "def load_all_groups(base_directory):\n",
    "    groups = [\"positive_group\", \"negative_group\", \"validation_group\"]\n",
    "    data_frames = {}\n",
    "    for group in groups:\n",
    "        data_frames[group] = load_data(base_directory, group)\n",
    "    return data_frames\n",
    "\n",
    "def prepare_datasets(data_frames):\n",
    "    data_positive = data_frames[\"positive_group\"]\n",
    "    data_negative = data_frames[\"negative_group\"]\n",
    "    validation_data = data_frames[\"validation_group\"]  # Using provided validation data directly\n",
    "    validation_data.to_csv(f\"{base_directory}/validation_set.csv\", index=False)\n",
    "\n",
    "\n",
    "    # Combine positive and negative data, excluding validation data\n",
    "    combined_data = pd.concat([data_positive, data_negative])\n",
    "    total_positives = combined_data[combined_data['group'] == 'positive_group'].shape[0]\n",
    "    total_negatives = combined_data[combined_data['group'] == 'negative_group'].shape[0]\n",
    "    print(f\"Combined data count: {combined_data.shape[0]} rows (Positive: {total_positives}, Negative: {total_negatives})\")\n",
    "\n",
    "    # Splitting combined data into training and testing sets (70/30 split)\n",
    "    train_data, test_data = train_test_split(combined_data, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Checking if total rows in train and test match the combined data rows\n",
    "    total_train_test_rows = train_data.shape[0] + test_data.shape[0]\n",
    "    print(f\"Total rows in train + test: {total_train_test_rows} rows. Matches combined data: {total_train_test_rows == combined_data.shape[0]}\")\n",
    "\n",
    "    # Separating positive and negative data within training and testing sets\n",
    "    train_pos = train_data[train_data['group'] == 'positive_group']\n",
    "    train_neg = train_data[train_data['group'] == 'negative_group']\n",
    "    test_pos = test_data[test_data['group'] == 'positive_group']\n",
    "    test_neg = test_data[test_data['group'] == 'negative_group']\n",
    "\n",
    "    # Calculating distributions\n",
    "    train_pos_pct = (train_pos.shape[0] / train_data.shape[0]) * 100\n",
    "    train_neg_pct = (train_neg.shape[0] / train_data.shape[0]) * 100\n",
    "    test_pos_pct = (test_pos.shape[0] / test_data.shape[0]) * 100\n",
    "    test_neg_pct = (test_neg.shape[0] / test_data.shape[0]) * 100\n",
    "\n",
    "    print(f\"Distribution in training set — Positive: {train_pos_pct:.2f}%, Negative: {train_neg_pct:.2f}%\")\n",
    "    print(f\"Distribution in testing set — Positive: {test_pos_pct:.2f}%, Negative: {test_neg_pct:.2f}%\")\n",
    "\n",
    "    # Ensuring both sets contain positive and negative data (following the natural distribution)\n",
    "    train_data = pd.concat([train_pos, train_neg])\n",
    "    test_data = pd.concat([test_pos, test_neg])\n",
    "\n",
    "    print(f\"Final Training set: {train_data.shape[0]} rows (Positive: {train_pos.shape[0]}, Negative: {train_neg.shape[0]})\")\n",
    "    print(f\"Final Testing set: {test_data.shape[0]} rows (Positive: {test_pos.shape[0]}, Negative: {test_neg.shape[0]})\")\n",
    "    print(f\"Validation set: {validation_data.shape[0]} rows\")\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def shuffle_and_save(data, file_path):\n",
    "    # Shuffle data\n",
    "    column_headers = data.columns\n",
    "    shuffled_data = data.sample(frac=1, random_state=42)\n",
    "    # Save to TSV\n",
    "    shuffled_data.to_csv(file_path, index=False, header=column_headers)\n",
    "\n",
    "\n",
    "base_directory = '/mnt/sdb/markus-bsc-thesis-data/machine-learning'\n",
    "data_frames = load_all_groups(base_directory)\n",
    "training_data, testing_data = prepare_datasets(data_frames)\n",
    "\n",
    "shuffle_and_save(training_data, f\"{base_directory}/training_set.csv\")\n",
    "shuffle_and_save(testing_data, f\"{base_directory}/testing_set.csv\")"
   ],
   "id": "5188f1506ed319e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## XGBoost",
   "id": "82d9f5b9aaaec637"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "90c67693245c0f57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, log_loss, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "# Define the base directory and paths\n",
    "base_directory = '/mnt/sdb/markus-bsc-thesis-data/machine-learning'\n",
    "training_file = f\"{base_directory}/training_set.csv\"\n",
    "validation_file = f\"{base_directory}/validation_set.csv\"\n",
    "testing_file = f\"{base_directory}/testing_set.csv\"\n",
    "\n",
    "# Function to load and prepare the data\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    # Convert group to numeric labels: assuming 'positive_group' as 1, 'negative_group' as 0\n",
    "    label_mapping = {'positive_group': 1, 'negative_group': 0, 'validation_group': 2}  # Update as per actual data\n",
    "    data['label'] = data['group'].map(label_mapping)\n",
    "    data.drop(['group'], axis=1, inplace=True)  # Remove 'group' column from features\n",
    "    return data\n",
    "\n",
    "# Load datasets\n",
    "training_data = load_data(training_file)\n",
    "validation_data = load_data(validation_file)\n",
    "testing_data = load_data(testing_file)\n",
    "\n",
    "# Prepare XGBoost DMatrices\n",
    "dtrain = xgb.DMatrix(training_data.drop('label', axis=1), label=training_data['label'])\n",
    "dval = xgb.DMatrix(validation_data.drop('label', axis=1), label=validation_data['label'])\n",
    "dtest = xgb.DMatrix(testing_data.drop('label', axis=1), label=testing_data['label'])\n",
    "\n",
    "# Define XGBoost model parameters\n",
    "params = {\n",
    "    'max_depth': 7,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eta': 0.1,\n",
    "    'eval_metric': 'logloss',\n",
    "    'random_state': 42\n",
    "}\n",
    "num_rounds = 100\n",
    "\n",
    "# Initialize an empty dictionary to store evaluation results\n",
    "evals_result = {}\n",
    "\n",
    "# Initialize lists to store classification error for each dataset\n",
    "train_error = []\n",
    "val_error = []\n",
    "test_error = []\n",
    "\n",
    "# Train the model\n",
    "model = xgb.train(params, dtrain, num_rounds, evals=[(dtrain, 'train'), (dtest, 'test')], evals_result=evals_result)\n",
    "\n",
    "# Predictions and evaluation\n",
    "predictions_proba = model.predict(dtest)\n",
    "predictions = [1 if p >= 0.4 else 0 for p in predictions_proba]\n",
    "accuracy = accuracy_score(testing_data['label'], predictions)\n",
    "precision = precision_score(testing_data['label'], predictions)\n",
    "recall = recall_score(testing_data['label'], predictions)\n",
    "f1 = f1_score(testing_data['label'], predictions)\n",
    "logloss = log_loss(testing_data['label'], predictions_proba)\n",
    "roc_auc = roc_auc_score(testing_data['label'], predictions_proba)  # Calculate ROC AUC\n",
    "pr_auc = average_precision_score(testing_data['label'], predictions_proba)  # Calculate PR AUC\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Log Loss: {logloss}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "print(f\"PR AUC: {pr_auc}\")\n",
    "\n",
    "# Plotting feature importance and decision tree\n",
    "xgb.plot_importance(model)\n",
    "fig, ax = plt.subplots(figsize=(20, 10), dpi=300)\n",
    "xgb.plot_tree(model, num_trees=10, ax=ax)\n",
    "plt.title('XGBoost Tree')\n",
    "plt.savefig('xgb_tree_high_res.png', dpi=300)\n",
    "plt.show()\n"
   ],
   "id": "981c657e29654362",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define XGBoost model parameters\n",
    "params = {\n",
    "    'max_depth': 7,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eta': 0.1,\n",
    "    'eval_metric': 'logloss',\n",
    "    'random_state': 42\n",
    "}\n",
    "num_rounds = 100\n",
    "\n",
    "# Initialize an empty dictionary to store evaluation results\n",
    "evals_result = {}\n",
    "\n",
    "# Train the model\n",
    "model = xgb.train(params, dtrain, num_rounds,\n",
    "                  evals=[(dtrain, 'train'), (dval, 'validate')],\n",
    "                  evals_result=evals_result)\n",
    "\n",
    "# Plot learning curves\n",
    "epochs = range(1, num_rounds + 1)\n",
    "train_logloss = evals_result['train']['logloss']\n",
    "val_logloss = evals_result['validate']['logloss']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_logloss, label='Train Log Loss')\n",
    "plt.plot(epochs, val_logloss, label='Validation Log Loss')\n",
    "plt.title('Training and Validation Log Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('training_and_validation_log_loss_over_epochs.png', dpi=300)\n",
    "plt.show()\n"
   ],
   "id": "fd4863b0fc15048",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt  # Ensure plt is imported\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(testing_data['label'], predictions)\n",
    "\n",
    "# Create a heatmap from the confusion matrix\n",
    "plt.figure(figsize=(10, 7))  # Optional: Specify the figure size\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('conf_mtx.png', dpi=300)  # Save the figure before showing it\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "id": "438bd63a63c10b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt  # Ensure plt is imported\n",
    "\n",
    "# Compute precision-recall pairs for different probability thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(testing_data['label'], predictions_proba)\n",
    "\n",
    "# Create the plot for the precision-recall curve\n",
    "plt.figure(figsize=(8, 6))  # Specify figure size\n",
    "plt.plot(recall, precision, marker='.', label='Precision-Recall curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()  # Optional: Add a legend if you have multiple lines\n",
    "\n",
    "# Save the figure before displaying it\n",
    "plt.savefig('precision_recall_curve.png', dpi=600)  # Save the figure with high DPI\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Calculate the average precision score\n",
    "average_precision = average_precision_score(testing_data['label'], predictions_proba)\n",
    "print(f\"Average Precision-Recall Score: {average_precision}\")"
   ],
   "id": "bb871f3ed16b5fed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt  # Make sure plt is imported\n",
    "\n",
    "# Compute False Positive Rate, True Positive Rate, and thresholds\n",
    "fpr, tpr, thresholds = roc_curve(testing_data['label'], predictions_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Create a new figure for the ROC curve\n",
    "plt.figure(figsize=(8, 6))  # You can specify the figure size as needed\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal line for no-skill classifier\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Save the figure before showing it\n",
    "plt.savefig('roc.png', dpi=600)  # Save the figure with high DPI to ensure high resolution\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "id": "62e77ae3a415af81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Assuming the model is already trained and the validation_data is loaded\n",
    "# Prepare XGBoost DMatrix for validation data\n",
    "# Ensure only the original features used during training are in the DMatrix\n",
    "original_features = ['IMPACT', 'QUAL', 'DP', 'QD', 'MAX_AF']  # List all feature names used during training\n",
    "dval = xgb.DMatrix(validation_data[original_features])\n",
    "\n",
    "# Predict probabilities on the validation dataset\n",
    "validation_proba = model.predict(dval)\n",
    "validation_data['predicted_proba'] = validation_proba  # Add probabilities to validation data\n",
    "\n",
    "# Applying a threshold to classify predictions\n",
    "validation_predictions = [1 if p >= 0.38 else 0 for p in validation_proba]\n",
    "validation_data['predicted_label'] = validation_predictions  # Add predictions as a new column\n",
    "\n",
    "# Filter out the positive predictions\n",
    "positive_predictions = validation_data[validation_data['predicted_label'] == 1]\n",
    "\n",
    "# Save the positive predictions to a CSV file\n",
    "output_path = '/mnt/sdb/markus-bsc-thesis-data/machine-learning/positive_predictions.csv'\n",
    "positive_predictions[original_features + ['predicted_proba', 'predicted_label']].to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Exported {len(positive_predictions)} positive predictions to CSV.\")\n"
   ],
   "id": "936d3cfe0298b3d8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
